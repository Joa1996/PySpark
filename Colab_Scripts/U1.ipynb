{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkZwMVgDBH83",
        "outputId": "2c390410-ba41-403d-bc3d-8d891b4b23ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install requests\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pyspark --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EtVeYZKeB4SJ",
        "outputId": "066035f3-1e4d-4037-fc19-5efa4180894d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: ./bin/pyspark [options]\n",
            "\n",
            "Options:\n",
            "  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,\n",
            "                              k8s://https://host:port, or local (Default: local[*]).\n",
            "  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (\"client\") or\n",
            "                              on one of the worker machines inside the cluster (\"cluster\")\n",
            "                              (Default: client).\n",
            "  --class CLASS_NAME          Your application's main class (for Java / Scala apps).\n",
            "  --name NAME                 A name of your application.\n",
            "  --jars JARS                 Comma-separated list of jars to include on the driver\n",
            "                              and executor classpaths.\n",
            "  --packages                  Comma-separated list of maven coordinates of jars to include\n",
            "                              on the driver and executor classpaths. Will search the local\n",
            "                              maven repo, then maven central and any additional remote\n",
            "                              repositories given by --repositories. The format for the\n",
            "                              coordinates should be groupId:artifactId:version.\n",
            "  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while\n",
            "                              resolving the dependencies provided in --packages to avoid\n",
            "                              dependency conflicts.\n",
            "  --repositories              Comma-separated list of additional remote repositories to\n",
            "                              search for the maven coordinates given with --packages.\n",
            "  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place\n",
            "                              on the PYTHONPATH for Python apps.\n",
            "  --files FILES               Comma-separated list of files to be placed in the working\n",
            "                              directory of each executor. File paths of these files\n",
            "                              in executors can be accessed via SparkFiles.get(fileName).\n",
            "  --archives ARCHIVES         Comma-separated list of archives to be extracted into the\n",
            "                              working directory of each executor.\n",
            "\n",
            "  --conf, -c PROP=VALUE       Arbitrary Spark configuration property.\n",
            "  --properties-file FILE      Path to a file from which to load extra properties. If not\n",
            "                              specified, this will look for conf/spark-defaults.conf.\n",
            "\n",
            "  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n",
            "  --driver-java-options       Extra Java options to pass to the driver.\n",
            "  --driver-library-path       Extra library path entries to pass to the driver.\n",
            "  --driver-class-path         Extra class path entries to pass to the driver. Note that\n",
            "                              jars added with --jars are automatically included in the\n",
            "                              classpath.\n",
            "\n",
            "  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n",
            "\n",
            "  --proxy-user NAME           User to impersonate when submitting the application.\n",
            "                              This argument does not work with --principal / --keytab.\n",
            "\n",
            "  --help, -h                  Show this help message and exit.\n",
            "  --verbose, -v               Print additional debug output.\n",
            "  --version,                  Print the version of current Spark.\n",
            "\n",
            " Spark Connect only:\n",
            "   --remote CONNECT_URL       URL to connect to the server for Spark Connect, e.g.,\n",
            "                              sc://host:port. --master and --deploy-mode cannot be set\n",
            "                              together with this option. This option is experimental, and\n",
            "                              might change between minor releases.\n",
            "\n",
            " Cluster deploy mode only:\n",
            "  --driver-cores NUM          Number of cores used by the driver, only in cluster mode\n",
            "                              (Default: 1).\n",
            "\n",
            " Spark standalone or Mesos with cluster deploy mode only:\n",
            "  --supervise                 If given, restarts the driver on failure.\n",
            "\n",
            " Spark standalone, Mesos or K8s with cluster deploy mode only:\n",
            "  --kill SUBMISSION_ID        If given, kills the driver specified.\n",
            "  --status SUBMISSION_ID      If given, requests the status of the driver specified.\n",
            "\n",
            " Spark standalone and Mesos only:\n",
            "  --total-executor-cores NUM  Total cores for all executors.\n",
            "\n",
            " Spark standalone, YARN and Kubernetes only:\n",
            "  --executor-cores NUM        Number of cores used by each executor. (Default: 1 in\n",
            "                              YARN and K8S modes, or all available cores on the worker\n",
            "                              in standalone mode).\n",
            "\n",
            " Spark on YARN and Kubernetes only:\n",
            "  --num-executors NUM         Number of executors to launch (Default: 2).\n",
            "                              If dynamic allocation is enabled, the initial number of\n",
            "                              executors will be at least NUM.\n",
            "  --principal PRINCIPAL       Principal to be used to login to KDC.\n",
            "  --keytab KEYTAB             The full path to the file that contains the keytab for the\n",
            "                              principal specified above.\n",
            "\n",
            " Spark on YARN only:\n",
            "  --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\n",
            "      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /proc/meminfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yHEmIGUqHFKB",
        "outputId": "910cb5cc-49f4-409e-bbfe-a0c3ed3505e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MemTotal:       13290460 kB\n",
            "MemFree:         8192256 kB\n",
            "MemAvailable:   12105368 kB\n",
            "Buffers:          420820 kB\n",
            "Cached:          3615472 kB\n",
            "SwapCached:            0 kB\n",
            "Active:           852396 kB\n",
            "Inactive:        3895708 kB\n",
            "Active(anon):       1312 kB\n",
            "Inactive(anon):   712200 kB\n",
            "Active(file):     851084 kB\n",
            "Inactive(file):  3183508 kB\n",
            "Unevictable:           8 kB\n",
            "Mlocked:               8 kB\n",
            "SwapTotal:             0 kB\n",
            "SwapFree:              0 kB\n",
            "Dirty:               196 kB\n",
            "Writeback:             0 kB\n",
            "AnonPages:        711892 kB\n",
            "Mapped:           326548 kB\n",
            "Shmem:              1692 kB\n",
            "KReclaimable:     203924 kB\n",
            "Slab:             249740 kB\n",
            "SReclaimable:     203924 kB\n",
            "SUnreclaim:        45816 kB\n",
            "KernelStack:        5840 kB\n",
            "PageTables:        11932 kB\n",
            "SecPageTables:         0 kB\n",
            "NFS_Unstable:          0 kB\n",
            "Bounce:                0 kB\n",
            "WritebackTmp:          0 kB\n",
            "CommitLimit:     6645228 kB\n",
            "Committed_AS:    3038168 kB\n",
            "VmallocTotal:   34359738367 kB\n",
            "VmallocUsed:       12200 kB\n",
            "VmallocChunk:          0 kB\n",
            "Percpu:             1144 kB\n",
            "HardwareCorrupted:     0 kB\n",
            "AnonHugePages:         0 kB\n",
            "ShmemHugePages:        0 kB\n",
            "ShmemPmdMapped:        0 kB\n",
            "FileHugePages:         0 kB\n",
            "FilePmdMapped:         0 kB\n",
            "CmaTotal:              0 kB\n",
            "CmaFree:               0 kB\n",
            "Unaccepted:            0 kB\n",
            "HugePages_Total:       0\n",
            "HugePages_Free:        0\n",
            "HugePages_Rsvd:        0\n",
            "HugePages_Surp:        0\n",
            "Hugepagesize:       2048 kB\n",
            "Hugetlb:               0 kB\n",
            "DirectMap4k:       70456 kB\n",
            "DirectMap2M:     5169152 kB\n",
            "DirectMap1G:    10485760 kB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def descargar_y_descomprimir_anidados(url, nombre_archivo):\n",
        "    \"\"\"Descarga un archivo desde una URL y descomprime archivos ZIP anidados.\"\"\"\n",
        "    try:\n",
        "        respuesta = requests.get(url, stream=True)\n",
        "        respuesta.raise_for_status()\n",
        "\n",
        "        with open(nombre_archivo, 'wb') as archivo:\n",
        "            for bloque in respuesta.iter_content(chunk_size=8192):\n",
        "                archivo.write(bloque)\n",
        "        print(f\"Descarga exitosa: {nombre_archivo}\")\n",
        "\n",
        "        # Descomprimir el ZIP principal\n",
        "        if nombre_archivo.lower().endswith('.zip'):\n",
        "            with zipfile.ZipFile(nombre_archivo, 'r') as archivo_zip:\n",
        "                archivo_zip.extractall(path=os.path.dirname(nombre_archivo))\n",
        "            print(f\"Archivo ZIP principal descomprimido: {nombre_archivo}\")\n",
        "\n",
        "            # Descomprimir los ZIPs anidados\n",
        "            directorio_base = os.path.dirname(nombre_archivo)\n",
        "            patron_zip_anidados = os.path.join(directorio_base, '*.zip')\n",
        "            archivos_zip_anidados = glob.glob(patron_zip_anidados)\n",
        "\n",
        "            for zip_anidado in archivos_zip_anidados:\n",
        "                if zip_anidado != nombre_archivo: # Evitar procesar el archivo zip principal de nuevo\n",
        "                    with zipfile.ZipFile(zip_anidado, 'r') as archivo_zip_anidado:\n",
        "                        archivo_zip_anidado.extractall(path=directorio_base)\n",
        "                    print(f\"Archivo ZIP anidado descomprimido: {zip_anidado}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error durante la descarga: {e}\")\n",
        "    except zipfile.BadZipFile:\n",
        "        print(f\"Error: {nombre_archivo} o uno de los archivos zip internos no es un archivo zip válido.\")\n",
        "    except IOError as e:\n",
        "        print(f\"Error de E/S al escribir o descomprimir el archivo: {e}\")\n",
        "\n",
        "# Ejemplo de uso\n",
        "url_archivo = \"https://bit.ly/1Aoywaq\" #@param {type:\"string\"}\n",
        "nombre_archivo_local = \"donation.zip\" #@param {type:\"string\"}\n",
        "\n",
        "descargar_y_descomprimir_anidados(url_archivo, nombre_archivo_local)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4BIKvr3HLzw",
        "outputId": "d744e841-03bd-4d0b-8b9b-ab2733fe4289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descarga exitosa: donation.zip\n",
            "Archivo ZIP principal descomprimido: donation.zip\n",
            "Archivo ZIP anidado descomprimido: block_4.zip\n",
            "Archivo ZIP anidado descomprimido: block_8.zip\n",
            "Archivo ZIP anidado descomprimido: block_2.zip\n",
            "Archivo ZIP anidado descomprimido: block_6.zip\n",
            "Archivo ZIP anidado descomprimido: block_3.zip\n",
            "Archivo ZIP anidado descomprimido: block_7.zip\n",
            "Archivo ZIP anidado descomprimido: block_5.zip\n",
            "Archivo ZIP anidado descomprimido: block_10.zip\n",
            "Archivo ZIP anidado descomprimido: block_9.zip\n",
            "Archivo ZIP anidado descomprimido: block_1.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = pyspark.sql.SparkSession.builder.appName(\"MySparkApp\").getOrCreate()\n",
        "\n",
        "# Now you can use the 'spark' variable to read the CSV file\n",
        "prev = spark.read.csv('block_*.csv')#llemos todos los csv que empiezan con \"block....\"\n",
        "prev.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9fKXnKfJmTm",
        "outputId": "49d011a4-8c15-4f47-fadd-ba0eef922c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
            "|  _c0|  _c1|         _c2|         _c3|         _c4|         _c5|    _c6|   _c7|   _c8|   _c9|   _c10|    _c11|\n",
            "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
            "| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
            "| 3148| 8326|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
            "|14055|94934|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
            "|33948|34740|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
            "|  946|71870|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
            "|64880|71676|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
            "|25739|45991|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
            "|62415|93584|           1|           ?|           1|           ?|      1|     1|     1|     1|      0|    TRUE|\n",
            "|27995|31399|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
            "| 4909|12238|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
            "|15161|16743|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
            "|31703|37310|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
            "|30213|36558|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
            "|56596|56630|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
            "|16481|21174|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
            "|32649|37094|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
            "|34268|37260|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
            "|66117|69253|           1|           ?|           1|           ?|      1|     1|     1|     1|      0|    TRUE|\n",
            "| 2771|31982|           1|           ?|           1|           ?|      0|     1|     1|     1|      1|    TRUE|\n",
            "|23557|29673|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
            "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este data set se muestra el match o coincidencia entre distintos pacientes, es decir pacientes que tienen algo en comun, ( nombre, edad, direecion, etc.) a cada coincidencia veremos el campo final con TRUE en caso de que coincida algo y false lo opuesto."
      ],
      "metadata": {
        "id": "a3IgxTFUxoYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parsed = spark.read.option(\"header\", \"true\").option(\"nullValue\",\"?\").option(\"inferSchema\", \"true\").csv(\"block_*.csv\")"
      ],
      "metadata": {
        "id": "Nqb2SQ3KMsxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el paso anterior creamos un objeto que parsea ( remplaza los nulos,etc.) el dataframe original y lo almacena en la variable \"parsed\""
      ],
      "metadata": {
        "id": "zvumyB7OEhBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "y9O2yaz-yAZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parsed.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOKtNLXcMtaW",
        "outputId": "f045cfb3-273e-45d4-be6b-7184c2230369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id_1: integer (nullable = true)\n",
            " |-- id_2: integer (nullable = true)\n",
            " |-- cmp_fname_c1: double (nullable = true)\n",
            " |-- cmp_fname_c2: double (nullable = true)\n",
            " |-- cmp_lname_c1: double (nullable = true)\n",
            " |-- cmp_lname_c2: double (nullable = true)\n",
            " |-- cmp_sex: integer (nullable = true)\n",
            " |-- cmp_bd: integer (nullable = true)\n",
            " |-- cmp_bm: integer (nullable = true)\n",
            " |-- cmp_by: integer (nullable = true)\n",
            " |-- cmp_plz: integer (nullable = true)\n",
            " |-- is_match: boolean (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ver las primeras 5 lineas"
      ],
      "metadata": {
        "id": "5jy7PT5FwtaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parsed.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKbQxaxrwwX-",
        "outputId": "e645c3bd-38ce-4783-ed36-4088d2d5b49a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
            "| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
            "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
            "| 3148| 8326|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
            "|14055|94934|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
            "|33948|34740|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
            "|  946|71870|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
            "|64880|71676|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
            "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parsed.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2gxzWFAxHEo",
        "outputId": "bfaf4c09-7bd1-4296-d668-3b8aa6118314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5749132"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostramos la cantidad de matches y no matches"
      ],
      "metadata": {
        "id": "eJ5Kf7Ip9_ve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "parsed.groupBy(\"is_match\").count().orderBy(col(\"count\").desc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II6MveCTBgji",
        "outputId": "28ce56eb-461c-4361-eefc-ccc9fdecdc9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+\n",
            "|is_match|  count|\n",
            "+--------+-------+\n",
            "|   false|5728201|\n",
            "|    true|  20931|\n",
            "+--------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resumen del DF"
      ],
      "metadata": {
        "id": "zWJ3vCcXFSTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary = parsed.describe()\n",
        "summary.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SL5eV25jFUYw",
        "outputId": "91f834c4-7ab3-4539-87a6-c6ed66d0d0e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+------------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+-------------------+\n",
            "|summary|              id_1|              id_2|      cmp_fname_c1|      cmp_fname_c2|       cmp_lname_c1|       cmp_lname_c2|            cmp_sex|             cmp_bd|             cmp_bm|            cmp_by|            cmp_plz|\n",
            "+-------+------------------+------------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+-------------------+\n",
            "|  count|           5749132|           5749132|           5748125|            103698|            5749132|               2464|            5749132|            5748337|            5748337|           5748337|            5736289|\n",
            "|   mean| 33324.48559643438| 66587.43558331935|0.7129024704425707| 0.900017671890335|0.31562781930763056|0.31841283153174366|  0.955001381078048|0.22446526708507172|0.48885529849763504|0.2227485966810923|0.00552866147434343|\n",
            "| stddev|23659.859374487987|23620.487613269706|0.3887583596162788|0.2713176105782331| 0.3342336339615803| 0.3685670662006655|0.20730111116897443|0.41722972238461925| 0.4998758236779003|0.4160909629831711|0.07414914925420013|\n",
            "|    min|                 1|                 6|               0.0|               0.0|                0.0|                0.0|                  0|                  0|                  0|                 0|                  0|\n",
            "|    max|             99980|            100000|               1.0|               1.0|                1.0|                1.0|                  1|                  1|                  1|                 1|                  1|\n",
            "+-------+------------------+------------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pasamos el resumen a un DF de pandas"
      ],
      "metadata": {
        "id": "eXcqjttEwgSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " summary_p = summary.toPandas()\n",
        " print(summary_p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv44HghgwkDX",
        "outputId": "a1a97a62-cf44-447d-a48f-72d90f5fc700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  summary                id_1                id_2        cmp_fname_c1  \\\n",
            "0   count             5749132             5749132             5748125   \n",
            "1    mean   33324.48559643438   66587.43558331935  0.7129024704425707   \n",
            "2  stddev  23659.859374487987  23620.487613269706  0.3887583596162788   \n",
            "3     min                   1                   6                 0.0   \n",
            "4     max               99980              100000                 1.0   \n",
            "\n",
            "         cmp_fname_c2         cmp_lname_c1         cmp_lname_c2  \\\n",
            "0              103698              5749132                 2464   \n",
            "1   0.900017671890335  0.31562781930763056  0.31841283153174366   \n",
            "2  0.2713176105782331   0.3342336339615803   0.3685670662006655   \n",
            "3                 0.0                  0.0                  0.0   \n",
            "4                 1.0                  1.0                  1.0   \n",
            "\n",
            "               cmp_sex               cmp_bd               cmp_bm  \\\n",
            "0              5749132              5748337              5748337   \n",
            "1    0.955001381078048  0.22446526708507172  0.48885529849763504   \n",
            "2  0.20730111116897443  0.41722972238461925   0.4998758236779003   \n",
            "3                    0                    0                    0   \n",
            "4                    1                    1                    1   \n",
            "\n",
            "               cmp_by              cmp_plz  \n",
            "0             5748337              5736289  \n",
            "1  0.2227485966810923  0.00552866147434343  \n",
            "2  0.4160909629831711  0.07414914925420013  \n",
            "3                   0                    0  \n",
            "4                   1                    1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "De esta manera podriamos trabajar con pandas tal como lo conocemos"
      ],
      "metadata": {
        "id": "5Xqq-KjJxMj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rN7FAs1Sjmm",
        "outputId": "4605e9fc-d56a-45cd-a65e-a16503ad3206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0] on linux\n",
            "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/03/18 22:06:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/03/18 22:06:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.5.5\n",
            "      /_/\n",
            "\n",
            "Using Python version 3.11.11 (main, Dec  4 2024 08:55:07)\n",
            "Spark context Web UI available at http://7aa07fa5a0a2:4041\n",
            "Spark context available as 'sc' (master = local[*], app id = local-1742335590783).\n",
            "SparkSession available as 'spark'.\n",
            ">>> myRange = spark.range(1000).toDF(\"number\")\n",
            ">>> print(myRange)\n",
            "DataFrame[number: bigint]\n",
            ">>> exit()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myRange = spark.range(1000).toDF(\"number\")\n",
        "myRange.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLdUr7BzTD2B",
        "outputId": "8e86699f-dd45-4efb-e060-8e9218bafdfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|number|\n",
            "+------+\n",
            "|     0|\n",
            "|     1|\n",
            "|     2|\n",
            "|     3|\n",
            "|     4|\n",
            "|     5|\n",
            "|     6|\n",
            "|     7|\n",
            "|     8|\n",
            "|     9|\n",
            "|    10|\n",
            "|    11|\n",
            "|    12|\n",
            "|    13|\n",
            "|    14|\n",
            "|    15|\n",
            "|    16|\n",
            "|    17|\n",
            "|    18|\n",
            "|    19|\n",
            "+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}